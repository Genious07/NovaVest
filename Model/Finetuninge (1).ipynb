{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers accelerate datasets peft bitsandbytes safetensors huggingface_hub trl evaluate\n"
      ],
      "metadata": {
        "id": "TZ7aXgCKMkww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = \"/content/fintune5.json\"  # upload your file here\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# data is a list of conversations (each is a list of {\"role\":..,\"content\":..})\n",
        "print(\"Number of conversations:\", len(data))\n",
        "# show first conversation to confirm shape\n",
        "print(data[0][:6])\n"
      ],
      "metadata": {
        "id": "pnLDY5hoMlnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "flat = []\n",
        "for conv in data:\n",
        "    # conv is a list of turn dicts\n",
        "    for i in range(len(conv)-1):\n",
        "        if conv[i].get(\"role\") == \"user\" and conv[i+1].get(\"role\") == \"assistant\":\n",
        "            user_msg = conv[i].get(\"content\",\"\").strip()\n",
        "            assistant_msg = conv[i+1].get(\"content\",\"\").strip()\n",
        "            if user_msg and assistant_msg:\n",
        "                flat.append({\"input\": user_msg, \"response\": assistant_msg})\n",
        "\n",
        "print(\"Flat examples:\", len(flat))\n",
        "# peek first 5\n",
        "for ex in flat[:5]:\n",
        "    print(\"-----\")\n",
        "    print(\"INPUT:\", ex[\"input\"])\n",
        "    print(\"RESPONSE:\", ex[\"response\"])\n"
      ],
      "metadata": {
        "id": "wH3fotH6Mnhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell (python)\n",
        "import json, gzip\n",
        "\n",
        "out_path = \"/content/fintune_flat.jsonl\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for ex in flat:\n",
        "        fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "print(\"Saved flat JSONL to\", out_path)\n"
      ],
      "metadata": {
        "id": "Kidz8elxMq0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"json\", data_files=out_path, split=\"train\")\n",
        "ds = ds.train_test_split(test_size=0.05, seed=42)\n",
        "print(ds)\n"
      ],
      "metadata": {
        "id": "zI-7N0fzMsiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "ex6v_yF5NVDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "BASE_MODEL = \"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n"
      ],
      "metadata": {
        "id": ""
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_mask_fn(examples):\n",
        "    inputs = examples[\"input\"]\n",
        "    responses = examples[\"response\"]\n",
        "    texts = [f\"User: {u}\\nAssistant: {r}\" for u, r in zip(inputs, responses)]\n",
        "    prompt_texts = [f\"User: {u}\\nAssistant:\" for u in inputs]\n",
        "\n",
        "    tokenized_full = tokenizer(texts, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "    tokenized_prompt = tokenizer(prompt_texts, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "    labels = tokenized_full[\"input_ids\"].copy()\n",
        "\n",
        "    # For each example, set label tokens corresponding to prompt to -100 so loss ignores them\n",
        "    for i, prompt_ids in enumerate(tokenized_prompt[\"input_ids\"]):\n",
        "        prompt_len = 0\n",
        "        # count how many tokens are not padding (token id != tokenizer.pad_token_id)\n",
        "        for tok_id in prompt_ids:\n",
        "            if tok_id != tokenizer.pad_token_id:\n",
        "                prompt_len += 1\n",
        "            else:\n",
        "                break\n",
        "        # mask prompt portion\n",
        "        for j in range(prompt_len):\n",
        "            labels[i][j] = -100\n",
        "\n",
        "    tokenized_full[\"labels\"] = labels\n",
        "    return tokenized_full\n",
        "\n",
        "tokenized = ds.map(tokenize_and_mask_fn, batched=True, batch_size=200,\n",
        "                   remove_columns=ds[\"train\"].column_names)\n",
        "print(tokenized[\"train\"][0])\n"
      ],
      "metadata": {
        "id": "hmEeO8w7M60Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell (python)\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# Load model (8-bit if necessary)\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "except Exception as e:\n",
        "    print(\"Float16 load failed, trying 8-bit (slower but fits small GPUs):\", e)\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "# If model loaded in k-bit:\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "Laq57nZ_M9Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate\n"
      ],
      "metadata": {
        "id": "KZKjPfjWOhsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell (python)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    report_to=[],\n",
        "    dataloader_pin_memory=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"test\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./qwen-lora\")\n"
      ],
      "metadata": {
        "id": "b00qO65OM_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_pretrained(\"\")   # adapter + config\n",
        "\n",
        "# Example quick test:\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "print(pipe(\"User: How do I start budgeting?\\nAssistant:\", max_new_tokens=120)[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "pyPZ390PNEFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
